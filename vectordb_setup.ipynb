{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"resume_data\", glob=\"*.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='[Jane Doe]\\n\\n[456 Mock Avenue]\\n\\n[City, State, ZIP]\\n\\n[Phone Number]\\n\\n[janedoe@email.com]\\n\\nObjective: Detail-oriented data analyst with a strong background in statistical analysis and data visualization. Seeking challenging opportunities to apply analytical skills and extract actionable insights from complex datasets to drive informed business decisions.\\n\\nEducation: Bachelor of Science in Statistics [University Name], [City, State] [Year of Graduation]\\n\\nSkills:\\n\\nProficient in statistical analysis software, including R and Python Strong knowledge of data visualization tools such as Tableau and Power BI Experience with SQL and database querying Excellent problem-solving and critical thinking abilities Strong attention to detail and accuracy in data analysis Effective communication and presentation skills Work Experience:\\n\\nData Analyst\\n\\nABC Consulting, [City, State]\\n\\n[Dates]\\n\\nConducted data cleaning, validation, and preprocessing to ensure data quality and integrity. Performed statistical analysis on large datasets, identifying trends and patterns to support business decisions. Created data visualizations and interactive dashboards using Tableau and Power BI, facilitating data-driven insights for stakeholders. Collaborated with cross-functional teams to define analytical objectives and deliver actionable recommendations. Conducted ad-hoc analyses and data mining to uncover business opportunities and provide strategic insights. Developed automated reports and streamlined data analysis processes, reducing analysis time by 20%. Presented findings and insights to senior management, effectively communicating complex concepts in a clear and concise manner. Data Analysis Intern XYZ Corporation, [City, State] [Dates]\\n\\nAssisted in analyzing customer behavior and market trends, providing insights for marketing campaigns and product development. Conducted data cleaning and preprocessing tasks, ensuring data accuracy and consistency. Collaborated with the team to develop statistical models and predictive analytics algorithms. Assisted in creating data visualizations and reports for internal and external stakeholders. Projects:\\n\\nSales Forecasting Analysis\\n\\nDeveloped a sales forecasting model using time series analysis to accurately predict future sales and optimize inventory management. Utilized R and statistical techniques such as ARIMA and exponential smoothing for accurate forecasting. Customer Segmentation Analysis\\n\\nConducted a customer segmentation analysis using clustering algorithms to identify distinct customer groups based on purchasing behavior and demographics. Utilized Python and scikit-learn for clustering analysis and visualized the results using Tableau.', metadata={'source': 'resume_data\\\\fake_3.txt'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# import sys\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available engines: \n",
      "['whisper-1', 'babbage', 'text-davinci-003', 'davinci', 'text-davinci-edit-001', 'babbage-code-search-code', 'text-similarity-babbage-001', 'code-davinci-edit-001', 'text-davinci-001', 'gpt-4-0613', 'ada', 'babbage-code-search-text', 'babbage-similarity', 'gpt-4', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k-0613', 'code-search-babbage-text-001', 'text-curie-001', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'code-search-babbage-code-001', 'text-ada-001', 'text-similarity-ada-001', 'curie-instruct-beta', 'gpt-3.5-turbo-0301', 'ada-code-search-code', 'ada-similarity', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'gpt-4-0314', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-embedding-ada-002', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available engines: \")\n",
    "print([data['id'] for data in openai.Engine.list()['data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding = embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_search = vectordb.similarity_search(sentence1, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[John Smith]\\n\\n[123 Fake Street]\\n\\n[City, State, ZIP]\\n\\n[Phone Number]\\n\\n[johnsmith@email.com]\\n\\nObjective: Results-driven data scientist with a passion for analyzing complex datasets and generating actionable insights. Seeking challenging opportunities to apply advanced statistical and machine learning techniques to solve real-world problems and drive business growth.\\n\\nEducation: Bachelor of Science in Computer Science [University Name], [City, State] [Year of Graduation]\\n\\nSkills:\\n\\nProficient in programming languages such as Python and R Strong knowledge of statistical analysis and machine learning algorithms Experience with data visualization tools, including Tableau and matplotlib Familiarity with SQL and database management Excellent problem-solving and analytical skills Strong communication and presentation abilities Work Experience:\\n\\nData Scientist\\n\\nXYZ Analytics, [City, State]\\n\\n[Dates]\\n\\nConducted exploratory data analysis on large datasets, identifying trends and patterns that informed business strategies and decision-making processes. Developed and implemented machine learning models for predictive analytics, resulting in a 15% increase in customer retention rate. Collaborated with cross-functional teams to define project goals, formulate hypotheses, and design experiments to validate findings. Cleaned and preprocessed data using Python and R, ensuring data integrity and quality. Created interactive data visualizations using Tableau and matplotlib, effectively communicating insights to stakeholders. Collaborated with software engineers to deploy machine learning models into production environments. Presented findings and actionable recommendations to senior leadership, facilitating data-driven decision-making. Data Science Intern ABC Tech Solutions, [City, State] [Dates]\\n\\nAssisted in the development of a recommendation engine using collaborative filtering techniques, improving customer engagement and increasing sales by 10%. Conducted data cleaning and preprocessing tasks, ensuring data accuracy and consistency. Developed and optimized predictive models using Python and R, providing valuable insights into customer behavior and market trends. Assisted in the design and implementation of A/B tests to evaluate the impact of different marketing strategies. Contributed to the development of data visualization dashboards for reporting and monitoring key metrics. Projects:\\n\\nCustomer Segmentation Analysis\\n\\nConducted a comprehensive customer segmentation analysis using clustering algorithms to identify distinct customer groups and tailor marketing strategies accordingly. Utilized Python, scikit-learn, and K-means clustering to categorize customers based on purchasing behavior and demographics. Sentiment Analysis of Social Media Data\\n\\nDeveloped a sentiment analysis model using natural language processing techniques to analyze social media data and identify trends in customer sentiment towards a specific brand or product. Leveraged Python, NLTK, and machine learning algorithms to classify social media posts as positive, negative, or neutral. References: Available upon request', metadata={'source': 'resume_data\\\\fake_1.txt'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.predict(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(resume_text):\n",
    "\n",
    "    template = \"\"\"You are a helpful assistant that extracts the applicant name from the resume. Only output the full name in this format:\\n \\\n",
    "    first_name, last_name \\n\\n \"\"\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "    human_template = \"Resume: \\n\\n {resume_text}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    # chat_prompt.format_messages(resume_text=\"resume_text\")\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "    name = chain.run(resume_text=resume_text)\n",
    "\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"resume_data/fake_2.txt\", \"r\") as f:\n",
    "    resume_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)).\n"
     ]
    }
   ],
   "source": [
    "name = get_name(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: resume_data\\fake_1.txt\n",
      "Loading file: resume_data\\fake_2.txt\n",
      "Loading file: resume_data\\fake_3.txt\n",
      "['John Smith', 'Kim Jong Un', 'Jane Doe']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = r\"resume_data\"\n",
    "\n",
    "name_list = []\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(path):\n",
    "    file_path = os.path.join(path, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f'Loading file: {file_path}')\n",
    "        # add your code here to load or process the file\n",
    "        # for example, if it's a text file you might do:\n",
    "        with open(file_path, 'r') as f:\n",
    "            resume_text = f.read()\n",
    "            name = get_name(resume_text)\n",
    "            name_list.append(name)\n",
    "\n",
    "print(name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
