{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "# import sys\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available engines: \n",
      "['whisper-1', 'babbage', 'text-davinci-003', 'davinci', 'text-davinci-edit-001', 'babbage-code-search-code', 'text-similarity-babbage-001', 'code-davinci-edit-001', 'text-davinci-001', 'ada', 'babbage-code-search-text', 'babbage-similarity', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k-0613', 'code-search-babbage-text-001', 'text-curie-001', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'code-search-babbage-code-001', 'text-ada-001', 'text-similarity-ada-001', 'curie-instruct-beta', 'gpt-3.5-turbo-0301', 'ada-code-search-code', 'ada-similarity', 'code-search-ada-text-001', 'text-search-ada-query-001', 'davinci-search-document', 'ada-code-search-text', 'text-search-ada-doc-001', 'davinci-instruct-beta', 'text-similarity-curie-001', 'code-search-ada-code-001', 'ada-search-query', 'text-search-davinci-query-001', 'curie-search-query', 'davinci-search-query', 'babbage-search-document', 'ada-search-document', 'text-search-curie-query-001', 'text-search-babbage-doc-001', 'curie-search-document', 'text-search-curie-doc-001', 'babbage-search-query', 'text-babbage-001', 'text-search-davinci-doc-001', 'text-search-babbage-query-001', 'curie-similarity', 'curie', 'text-embedding-ada-002', 'text-similarity-davinci-001', 'text-davinci-002', 'davinci-similarity']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available engines: \")\n",
    "print([data['id'] for data in openai.Engine.list()['data']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract resume names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(resume_text):\n",
    "\n",
    "    template = \"\"\"You are a helpful assistant that extracts the applicant name from the resume. Only output the full name in this format:\\n \\\n",
    "    first_name, last_name \\n\\n \"\"\"\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "    human_template = \"Resume: \\n\\n {resume_text}\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    # chat_prompt.format_messages(resume_text=\"resume_text\")\n",
    "    chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "    name = chain.run(resume_text=resume_text)\n",
    "\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_from_file(filename):\n",
    "    # assumes the filename is in the format: firstname_lastname_resume.txt\n",
    "    basename = os.path.splitext(filename)[0]\n",
    "    names = basename.split(\"_\")\n",
    "    full_name = names[0] + \" \" + names[1]\n",
    "    return full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abraham Lincoln\n",
      "Agatha Christie\n",
      "Alberto Santos-Dumont\n",
      "Amadeo Avogadro\n",
      "Andy Warhol\n",
      "Anne Frank\n",
      "Audrey Hepburn\n",
      "Barack Obama\n",
      "Che Guevara\n",
      "Cleopatra resume\n",
      "Coco Chanel\n",
      "Dalai Lama\n",
      "David Bowie\n",
      "Diego Maradona\n",
      "Elvis Presley\n",
      "Emily Brontë\n",
      "Eva Perón\n",
      "Fidel Castro\n",
      "Freddie Mercury\n",
      "Frederick Douglass\n",
      "Galileo Galilei\n",
      "George Orwell\n",
      "George Washington\n",
      "Helen Keller\n",
      "J.K. Rowling\n",
      "Jim Morrison\n",
      "Johannes Gutenberg\n",
      "John D.\n",
      "John F.\n",
      "John Lennon\n",
      "John Steinbeck\n",
      "Joseph Stalin\n",
      "Julius Caesar\n",
      "Kurt Cobain\n",
      "Leonardo da\n",
      "Leon Trotsky\n",
      "Leo Tolstoy\n",
      "Louis Pasteur\n",
      "Mahatma Gandhi\n",
      "Mao Zedong\n",
      "Marie Antoinette\n",
      "Marie Curie\n",
      "Marlon Brando\n",
      "Martin Luther\n",
      "Michael Jordan\n",
      "Mikhail Gorbachev\n",
      "Muhammad Ali\n",
      "Nelson Mandela\n",
      "Nikola Tesla\n",
      "Oprah Winfrey\n",
      "Pierre-Auguste Renoir\n",
      "Pierre Curie\n",
      "Plato resume\n",
      "Pope Francis\n",
      "Princess Diana\n",
      "Queen Elizabeth\n",
      "Roger Federer\n",
      "Rosalind Franklin\n",
      "Rosa Parks\n",
      "Stephen Hawking\n",
      "Steve Jobs\n",
      "Thomas Edison\n",
      "Virginia Woolf\n",
      "Vladimir Lenin\n",
      "Walt Disney\n",
      "Winston Churchill\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = r\"resume_data\"\n",
    "metadata_list = []\n",
    "\n",
    "name_list = []\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(path):\n",
    "    file_path = os.path.join(path, filename)\n",
    "    if os.path.isfile(file_path) and (filename != \".DS_Store\"):\n",
    "        # print(f'Loading file: {file_path}')\n",
    "        full_name = get_name_from_file(filename)\n",
    "        print(full_name)\n",
    "        metadata = {\"filename\": filename, \"full_name\": full_name}\n",
    "        metadata_list.append(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load resume_data via DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a valid json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/unstructured/partition/json.py:40\u001b[0m, in \u001b[0;36mpartition_json\u001b[0;34m(filename, file, text)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(file_text)\n\u001b[1;32m     41\u001b[0m     elements \u001b[39m=\u001b[39m dict_to_elements(\u001b[39mdict\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m DirectoryLoader\n\u001b[1;32m      2\u001b[0m loader \u001b[39m=\u001b[39m DirectoryLoader(\u001b[39m\"\u001b[39m\u001b[39mresume_data\u001b[39m\u001b[39m\"\u001b[39m, glob\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/document_loaders/directory.py:131\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m items:\n\u001b[0;32m--> 131\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_file(i, p, docs, pbar)\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m pbar:\n\u001b[1;32m    134\u001b[0m     pbar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/document_loaders/directory.py:92\u001b[0m, in \u001b[0;36mDirectoryLoader.load_file\u001b[0;34m(self, item, path, docs, pbar)\u001b[0m\n\u001b[1;32m     90\u001b[0m         logger\u001b[39m.\u001b[39mwarning(e)\n\u001b[1;32m     91\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     93\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m pbar:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/document_loaders/directory.py:86\u001b[0m, in \u001b[0;36mDirectoryLoader.load_file\u001b[0;34m(self, item, path, docs, pbar)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m _is_visible(item\u001b[39m.\u001b[39mrelative_to(path)) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_hidden:\n\u001b[1;32m     85\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         sub_docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_cls(\u001b[39mstr\u001b[39;49m(item), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader_kwargs)\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     87\u001b[0m         docs\u001b[39m.\u001b[39mextend(sub_docs)\n\u001b[1;32m     88\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/document_loaders/unstructured.py:71\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[1;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     73\u001b[0m         docs: List[Document] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/document_loaders/unstructured.py:133\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_elements\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m    131\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39munstructured\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpartition\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m partition\n\u001b[0;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m partition(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/unstructured/partition/auto.py:180\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, ssl_verify, ocr_languages, pdf_infer_table_structure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     elements \u001b[39m=\u001b[39m partition_pptx(\n\u001b[1;32m    175\u001b[0m         filename\u001b[39m=\u001b[39mfilename,\n\u001b[1;32m    176\u001b[0m         file\u001b[39m=\u001b[39mfile,\n\u001b[1;32m    177\u001b[0m         include_page_breaks\u001b[39m=\u001b[39minclude_page_breaks,\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m \u001b[39melif\u001b[39;00m filetype \u001b[39m==\u001b[39m FileType\u001b[39m.\u001b[39mJSON:\n\u001b[0;32m--> 180\u001b[0m     elements \u001b[39m=\u001b[39m partition_json(filename\u001b[39m=\u001b[39;49mfilename, file\u001b[39m=\u001b[39;49mfile)\n\u001b[1;32m    181\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInvalid file\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid file \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/unstructured/partition/json.py:43\u001b[0m, in \u001b[0;36mpartition_json\u001b[0;34m(filename, file, text)\u001b[0m\n\u001b[1;32m     41\u001b[0m     elements \u001b[39m=\u001b[39m dict_to_elements(\u001b[39mdict\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot a valid json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39m# NOTE(Nathan): in future PR, try extracting items that look like text\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m#               if file_text is a valid json but not an unstructured json\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m elements\n",
      "\u001b[0;31mValueError\u001b[0m: Not a valid json"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"resume_data\", glob=\"*.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mlen\u001b[39m(docs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'resume_data\\\\Coco_Chanel_resume.txt'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [f\"resume_{str(i)}\" for i in range(1, len(docs)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# create the vectorstore\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(\n\u001b[0;32m----> 3\u001b[0m     documents \u001b[39m=\u001b[39m docs,\n\u001b[1;32m      4\u001b[0m     embedding \u001b[39m=\u001b[39m embedding_function,\n\u001b[1;32m      5\u001b[0m     collection_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfull_resume\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     persist_directory\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mchroma/full_resume/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     ids\u001b[39m=\u001b[39mids\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "# create the vectorstore\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents = docs,\n",
    "    embedding = embedding_function,\n",
    "    collection_name=\"full_resume\",\n",
    "    persist_directory='chroma/full_resume/',\n",
    "    ids=ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update docs with metadata_list\n",
    "for i in range(len(docs)):\n",
    "    docs[i].metadata = metadata_list[i]\n",
    "    vectordb.update_document(ids[i], docs[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## similarity search demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Machine learning engineer\"\n",
    "results = vectordb.similarity_search_with_score(\"software engineer with aws experience\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m score \u001b[39m=\u001b[39m results[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "score = results[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    \n",
    "    print(f'Metadata: ', result[0].metadata)\n",
    "    print('Score: ',result[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
