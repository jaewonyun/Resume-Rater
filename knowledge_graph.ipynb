{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from util import extract_text_from_resume\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the resumes\n",
    "resume_dir = \"/Users/yunjaewon/ChatGPT/resumes/\"\n",
    "resume_list=os.listdir(resume_dir)\n",
    "print(resume_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = extract_text_from_resume(resume_dir+'jaeDE.pdf')\n",
    "#print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Extraction Trial (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string=\"\"\"Extract skills from a resume \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "print(prompt_template.messages[0].prompt)\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_style=\"\"\"Python List\"\"\"\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=result_style,\n",
    "                    text=resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "result = chat(customer_messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = textwrap.TextWrapper(width=100)  # change 50 to any number that suits your needs\n",
    "word_list = wrapper.wrap(text=result.content)\n",
    "\n",
    "for line in word_list:\n",
    "    print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1: Extracting everythin at once\n",
    "Problem: only extract first project from Branchy solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_schema=ResponseSchema(name=\"contact\",\n",
    "description=\"Extracts personal contact information in JSON format. \\\n",
    "    The key values include 'name', 'email', 'phone number', and personal websites such as 'LinkedIn' or 'Github profiles'.\")\n",
    "\n",
    "education_schema=ResponseSchema(name=\"education\",\n",
    "description=\"Extracts information about educational background or degrees in JSON format.\\\n",
    "      The key values include 'institutions', 'degree types', 'majors', and 'graduation dates'.\")\n",
    "\n",
    "experience_schema=ResponseSchema(name=\"experience\",\n",
    "description=\"Extracts work experience details in JSON format. \\\n",
    "    Key values include 'job title', 'employer name', 'employment duration', and 'job description/responsibilities'.\")\n",
    "\n",
    "skills_schema=ResponseSchema(name=\"skills\",\n",
    "description=\"Extracts details about professional and technical skills in JSON format. \\\n",
    "    The key values is 'skills' and the values are listed in a Python list, delimited by commas.\")\n",
    "\n",
    "response_schemas=[contact_schema, education_schema, experience_schema, skills_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: only extract first project from Branchy solution\n",
    "output_template=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "contact:Extracts personal contact information in JSON format. \\\n",
    "    The key values include 'name', 'email', 'phone number', and personal websites such as 'LinkedIn' or 'Github profiles'.\n",
    "\n",
    "education:Extracts information about educational background or degrees in JSON format.\\\n",
    "      The key values include 'institutions', 'degree types', 'majors', and 'graduation dates'.\n",
    "\n",
    "experience:Extracts work experience details in JSON format. \\\n",
    "    Key values include 'job title', 'employer name', 'employment duration', and 'job description/responsibilities'.\n",
    "\n",
    "skills:Extracts details about professional and technical skills in JSON format. \\\n",
    "    The key values is 'skills' and the values are listed in a Python list, delimited by commas.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=output_template)\n",
    "\n",
    "messages = prompt.format_messages(text=resume_text, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict.get('experience')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2: Separate extracting experiences\n",
    "Pretty Successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_schema=ResponseSchema(name=\"contact\",\n",
    "description=\"Extracts personal contact information in JSON format. \\\n",
    "    The keys are 'name', 'email', and 'phone number'. \\\n",
    "    The corresponding values for these keys should be in the form of Python lists, delimited by commas.\")\n",
    "\n",
    "education_schema=ResponseSchema(name=\"education\",\n",
    "description=\"Extracts information about educational background or degrees in JSON format.\\\n",
    "            The keys are 'institutions', 'degree types', 'majors', and 'graduation dates'.\\\n",
    "            The corresponding values for these keys should be in the form of Python lists, delimited by commas.\")\n",
    "\n",
    "skills_schema=ResponseSchema(name=\"skills\",\n",
    "description=\"Extracts details about professional and technical skills in JSON format. \\\n",
    "    The key is 'skills' and the values are listed in a Python list, delimited by commas.\")\n",
    "\n",
    "response_schemas1=[contact_schema, education_schema, skills_schema]\n",
    "\n",
    "work_experience_schema=ResponseSchema(name=\"work experiences\",\n",
    "description=\"Extracts work experience details and technical skills used in each work experience in JSON format. \\\n",
    "    the keys are 'job title', 'employer', 'employment duration', 'job description' and 'technical skills'.\\\n",
    "        The corresponding values for these keys should be in the form of Python lists, delimited by commas.\")\n",
    "\n",
    "project_schema=ResponseSchema(name=\"projects\",\n",
    "description=\"Extracts any project detail and technical skills used in each project in JSON format. \\\n",
    "    the keys are 'project name', 'project detail' and 'technical skills'.\\\n",
    "        The corresponding values for these keys should be in the form of Python lists, delimited by commas.\")\n",
    "\n",
    "response_schemas2=[work_experience_schema,project_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser1 = StructuredOutputParser.from_response_schemas(response_schemas1)\n",
    "format_instructions1 = output_parser1.get_format_instructions()\n",
    "\n",
    "output_parser2 = StructuredOutputParser.from_response_schemas(response_schemas2)\n",
    "format_instructions2 = output_parser2.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(format_instructions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(format_instructions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_template1=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "contact: Extracts personal contact information in JSON format. \\\n",
    "    The keys are 'name', 'email', and 'phone number'. \\\n",
    "    The corresponding values for these keys should be in the form of Python lists, delimited by commas.\n",
    "\n",
    "education: Extracts information about educational background or degrees in JSON format.\\\n",
    "            The keys are 'institutions', 'degree types', 'majors', and 'graduation dates'.\\\n",
    "            The corresponding values for these keys should be in the form of Python lists, delimited by commas.\n",
    "            \n",
    "skills: Extracts details about professional and technical skills in JSON format. \\\n",
    "    The key values is 'skills' and the values are listed in a Python list, delimited by commas.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions1} \"\"\"\n",
    "\n",
    "output_template2=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "work experiences: Extracts work experience details and technical skills used in each work experience in JSON format. \\\n",
    "    the keys are 'job title', 'employer', 'employment duration', 'job description' and 'technical skills'.\\\n",
    "        The corresponding values for these keys should be in the form of Python lists, delimited by commas.\n",
    "\n",
    "projects: Extracts any project detail and technical skills used in each project in JSON format. \\\n",
    "    the keys are 'project name', 'project detail' and 'technical skills'.\\\n",
    "        The corresponding values for these keys should be in the form of Python lists, delimited by commas.\n",
    "        \n",
    "text: {text}\n",
    "\n",
    "{format_instructions2} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(template=output_template1)\n",
    "\n",
    "messages1 = prompt1.format_messages(text=resume_text, \n",
    "                                format_instructions1=format_instructions1)\n",
    "prompt2 = ChatPromptTemplate.from_template(template=output_template2)\n",
    "\n",
    "messages2 = prompt2.format_messages(text=resume_text, \n",
    "                                format_instructions2=format_instructions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(messages1[0].content)\n",
    "#print(messages2[0].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = chat(messages1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=chat(messages2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict1 = output_parser1.parse(response1.content)\n",
    "output_dict2 = output_parser2.parse(response2.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict2.get('work experiences')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the matching ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_requirement={\"Minimum qualification\":\n",
    "                    {\"Degree\":\"Bachelor\",\n",
    "                     \"Major\":[\"Physics\", \"Computer Science\", \"Electrical Engineering\", \"Mathmetics\", \"Machine Learning\", \"Statistics\"],\n",
    "                     \"Skills\":[\"Statistical Analysis\"],\n",
    "                     \"Years of Experience\":\"3 years of experience\"},\n",
    "                \"Preferred qualification\":\n",
    "                    {\"Degree\":\"Phd\",\n",
    "                     \"Major\":[\"Computer Science\"],\n",
    "                     \"Skills\":[\"AWS\",\"Time Series Analysis\",\"Natural Language Processing\"],\n",
    "                     \"Years of Experience\":\"5 years of experience\"}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_info=output_dict1 | output_dict2\n",
    "resume_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ChatGPT for comparision : Fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial 1\n",
    "MinQualification_schema=ResponseSchema(name=\"Matching minimum qualification\",\n",
    "description=\"Compare items of 'Minimum qualification' from python dictionary: {job_requirement} and {resume_info}.\\\n",
    "            Then, extract only matching items in JSON format, \\\n",
    "            where the key is 'Matching minimum qualification'.\")\n",
    "\n",
    "PrefQualification_schema=ResponseSchema(name=\"Matching preferred qualification\",\n",
    "description=\"Compare items of 'Preferred qualification' from python dictionary: {job_requirement} and qualifications from {resume_info}. \\\n",
    "            Then, extract only matching items in JSON format, \\\n",
    "            where the key is 'Matching preferred qualification'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial 2\n",
    "MinQualification_schema=ResponseSchema(name=\"Matching minimum qualification\",\n",
    "description=\"Compare the 'Minimum qualification' elements from the Python dictionaries: {job_requirement} and {resume_info}.\\\n",
    "      Extract and present the common items in a JSON structure, where the corresponding key is 'Matching minimum qualification'.\")\n",
    "\n",
    "PrefQualification_schema=ResponseSchema(name=\"Matching preferred qualification\",\n",
    "description=\"Compare the 'Preferred qualification' elements from the Python dictionaries: {job_requirement} and {resume_info}.\\\n",
    "      Extract and present the common items in a JSON structure, where the corresponding key is 'Matching preferred qualification'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial 3\n",
    "MinQualification_schema=ResponseSchema(name=\"Matching minimum qualification\",\n",
    "description=\"Compare the 'Minimum qualification' elements from the Python dictionaries: {job_requirement} and {resume_info}.\\\n",
    "      Extract the common items in a JSON structure, where the corresponding key is 'Matching minimum qualification'.\")\n",
    "\n",
    "PrefQualification_schema=ResponseSchema(name=\"Matching preferred qualification\",\n",
    "description=\"Compare the 'Preferred qualification' elements from the Python dictionaries: {job_requirement} and {resume_info}.\\\n",
    "      Extract the common items in a JSON structure, where the corresponding key is 'Matching preferred qualification'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas=[MinQualification_schema, PrefQualification_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_template=\"\"\"\n",
    "    For the following resume information and job requirement python dictionary, extract the following information:\n",
    "    \n",
    "    Matching minimum qualification: Compare the 'Minimum qualification' elements from the Python dictionaries: {job_requirement} and {resume_info}.\\\n",
    "      Extract the common items in a JSON structure, where the corresponding key is 'Matching minimum qualification'.\n",
    "    \n",
    "    Matching preferred qualification: Compare the 'Preferred qualification' elements from the Python dictionaries: {job_requirement} and {resume_info}.\\\n",
    "      Extract the common items in a JSON structure, where the corresponding key is 'Matching preferred qualification'.\n",
    "\n",
    "    \n",
    "    {format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=rubric_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.format_messages(resume_info=resume_info,\n",
    "                                  job_requirement=job_requirement, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chat(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content) ## Wrong output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use langchain Agents for comparison : Fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wrong!\n",
    "agent.run(f\"\"\"Calculate the matching ratio between 'Matching minimum qualification' from {output_dict} and 'Minimum qualification' from {job_requirement}\"\"\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Langchain to create Knowledge  (Because of format instructions gpt misses some info!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alan.pdf', '.DS_Store', 'jaewon.docx', 'oldresume.docx', 'jaeDE.pdf', 'lorraine.pdf', 'jaewon.pdf', 'mlscientist.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from util import extract_text_from_resume\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "\n",
    "\n",
    "def wraptext(text, width=100):\n",
    "    wrapper = textwrap.TextWrapper(width=width)\n",
    "    word_list = wrapper.wrap(text=text)\n",
    "\n",
    "    for element in word_list:\n",
    "        print(element)\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# Directory containing the resumes\n",
    "resume_dir = \"/Users/yunjaewon/ChatGPT/resumes/\"\n",
    "resume_list=os.listdir(resume_dir)\n",
    "print(resume_list)\n",
    "\n",
    "resume_text = extract_text_from_resume(resume_dir+'jaeDE.pdf')\n",
    "#print(resume_text)\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAEWON YUN\n",
      "\n",
      "Data Engineer\n",
      "\n",
      "WW jaewon.yun@mail.utoronto.ca , 4387285493 @ Montreal, QC\n",
      "\n",
      "Professional Experience\n",
      "\n",
      "Data Engineer, Branchy Solution 2\n",
      "01/2022 - present | Toronto, Canada\n",
      "End-to-End AWS Cloud Migration:\n",
      "\n",
      "Designed and executed robust ETL pipelines using AWS Glue Studio, Lambda, and\n",
      "$3, leading to improved data volume handling and enhancing overall data\n",
      "infrastructure scalability.\n",
      "\n",
      "Utilized Databricks with PySpark for efficient data processing, streamlining data\n",
      "transformation and load processes.\n",
      "\n",
      "Constructed robust data modeling strategies within AWS Redshift and RDS,\n",
      "bolstering the maintainability and scalability of data warehouse systems.\n",
      "\n",
      "Managed IAM roles and security groups, strengthening data protection and ensuring\n",
      "consistent system reliability.\n",
      "\n",
      "Large Language Model-Powered Resume Rater Engine:\n",
      "\n",
      "Engineered a Resume Rater Engine using OpenAl's large language model for\n",
      "precise information extraction and candidate profiling, driving a 30% surge in\n",
      "successful matches.\n",
      "\n",
      "Employed Apache Airflow and Docker to automate workflows and bolster\n",
      "deployment scalability, enhancing system maintainability and performance.\n",
      "Integrated Neo4j graph database with machine learning for predictive analytics,\n",
      "ensuring accurate and intelligent applicant evaluation.\n",
      "\n",
      "NLP-Powered Job Recommendation Engine with AWS:\n",
      "\n",
      "Orchestrated an NLP-powered job recommendation system utilizing AWS\n",
      "SageMaker and EC2, contributing to a 45% increase in service usage.\n",
      "\n",
      "Leveraged APIs and Python for data transformation, storing data in $3, DynamoDB\n",
      "and RDS, which propelled the recommendation engine, leading to a 60% surge in\n",
      "source data volume.\n",
      "\n",
      "COVID Hospitalization Data Analysis Utilizing Microsoft Azure:\n",
      "\n",
      "Employed Synapse Analytics for proficient data modeling and comprehensive data\n",
      "warehouse construction, utilizing SQL Database for structured data storage.\n",
      "Leveraged Databricks with PySpark for efficient big data analytics and processing,\n",
      "enhancing data transformation and load processes.\n",
      "\n",
      "Constructed ETL pipelines using Data Factory and optimized data management and\n",
      "retrieval using Data Lake Storage and Blob Storage.\n",
      "\n",
      "Machine Learning Intern, University of Toronto - Miller Group @\n",
      "08/2022 - 03/2023 | Toronto, Canada\n",
      "\n",
      "Developed a physics-based deep learning model using Python and TensorFlow to\n",
      "study how atoms change shape in a high-speed electron microscope, by analyzing\n",
      "he patterns made by the electrons.\n",
      "\n",
      "Research Assistant, University of Toronto - Miller Group @\n",
      "05/2019 - 05/2020 | Toronto, Canada\n",
      "\n",
      "Led and completed several projects focused on Ultrafast Electron Diffraction,\n",
      "contributing valuable insights to the group's research initiatives.\n",
      "\n",
      "Conducted rigorous data analysis on the diffraction patterns using advanced\n",
      "statistical methods and machine learning techniques, improving the accuracy of\n",
      "he research findings.\n",
      "\n",
      "in LinkedIn\n",
      "\n",
      "© Github\n",
      "\n",
      "Education\n",
      "\n",
      "Physics Specialist (HBSc),\n",
      "University of Toronto\n",
      "2020 | Toronto, Canada\n",
      "\n",
      "Skills\n",
      "\n",
      "Data Engineering/ETL Tools\n",
      "* Data Modeling\n",
      "\n",
      "* Data Warehousing\n",
      "\n",
      "¢ PySpark\n",
      "\n",
      "* Cypher Query Language\n",
      "* Docker\n",
      "\n",
      "* Databricks\n",
      "\n",
      "* Airflow\n",
      "\n",
      "* Graph Database (Neo4)j)\n",
      "* Git\n",
      "\n",
      "* Python\n",
      "\n",
      "* SQL/NoSQL Databases\n",
      "* SQL\n",
      "\n",
      "Amazon Web Services\n",
      "¢ Redshift\n",
      "\n",
      "* SageMaker\n",
      "\n",
      "* Glue Studio\n",
      "\n",
      "° EC2\n",
      "\n",
      "* RDS\n",
      "\n",
      "¢ Lambda\n",
      "\n",
      "° $3\n",
      "\n",
      "* Cloud Development Kit\n",
      "* DynamoDB\n",
      "\n",
      "e IAM\n",
      "\n",
      "Microsoft Azure\n",
      "\n",
      "* Synapse Analytics\n",
      "¢ Data Lake Storage\n",
      "¢ Data Factory\n",
      "\n",
      "* SQL Database\n",
      "Blob Storage\n",
      "\n",
      "Machine Learning\n",
      "* Natural Language\n",
      "Processing\n",
      "\n",
      "¢ Recommendation System\n",
      "* Statistical Modeling\n",
      "\n",
      "* Medical Image Processing\n",
      "* Time Series Analysis\n",
      "\n",
      "« Knowledge Graph\n",
      "\n",
      "¢ TensorFlow/Keras\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_schema=ResponseSchema(name=\"name\",\n",
    "description=\"Extracts name in JSON format. \\\n",
    "    The keys must be 'name'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the name cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\")\n",
    "\n",
    "email_schema=ResponseSchema(name=\"email\",\n",
    "description=\"Extracts email in JSON format. \\\n",
    "    The keys must be 'email'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the email cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\")\n",
    "\n",
    "\n",
    "phone_number_schema=ResponseSchema(name=\"phone number\",\n",
    "description=\"Extracts phone number in JSON format. \\\n",
    "    The keys must be 'phone number'.Each corresponding value should be represented as a Python string. \\\n",
    "    If the phone number cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\")\n",
    "\n",
    "\n",
    "education_schema=ResponseSchema(name=\"education\",\n",
    "description=\"Extract information about the individual's educational background in JSON format.\\\n",
    "      Each educational experience should be represented as a separate JSON object. \\\n",
    "    For each education instance, the keys must be 'institution', 'degree_type', 'major', and 'graduation_date'. \\\n",
    "    Each corresponding value should be represented as a Python string.\\\n",
    "    If any information cannot be found for a given key, ensure the key is still included in the JSON object, but assign its corresponding value as null.\")\n",
    "\n",
    "skills_schema=ResponseSchema(name=\"skills\",\n",
    "description=\"Extracts details about professional and technical skills in JSON format. \\\n",
    "    The key must be 'skills'.\\\n",
    "        Each corresponding value should be represented as a Python list, with individual items separated by commas. \\\n",
    "    If certain information cannot be found, the corresponding key should still be included in the JSON object, but its value should be an empty Python list.\")\n",
    "\n",
    "response_schemas1=[name_schema, email_schema, phone_number_schema, education_schema, skills_schema]\n",
    "\n",
    "work_experience_schema=ResponseSchema(name=\"work experiences\",\n",
    "description=\"Follow steps below to extract work experiences: \\\n",
    "1. Begin by extracting details about each distinct job role from the work experience section.\\\n",
    "2. For every distinct job role, even if it is within the same company, create a separate JSON object.\\\n",
    "3. Each JSON object must include the following keys: 'job_title', 'employer', and 'employment_duration'.\\\n",
    "4. For the keys 'job_title', 'employer', and 'employment_duration', represent the corresponding values as Python strings.\\\n",
    "      For instance, the 'job_title' for a specific role might look like: 'Software Engineer'.\\\n",
    "5. If there is any key for which you cannot find the corresponding information, ensure that this key is still included in the JSON object. \\\n",
    "    If no details are found for the keys, assign their value as an empty Python string, for instance 'job_title': ''.\\\n",
    "6. Repeat these steps for each distinct job role identified in the work experience section.\")\n",
    "\n",
    "response_schemas2=[work_experience_schema]\n",
    "\n",
    "project_schema=ResponseSchema(name=\"projects\",\n",
    "description=\"Follow the steps below to extract project details:\\\n",
    "1. Start by identifying and extracting details for each distinct project, the employer, the job title, and the technical skills utilized in each project.\\\n",
    "2. For each distinct project, create a separate JSON object.\\\n",
    "3. The JSON object for each project must include the following keys: 'project_name', 'employer', 'job_title', and 'technical_skills'.\\\n",
    "4. Represent the corresponding values for each key as Python lists or strings. \\\n",
    "Each individual item within the 'technical_skills' list should be separated by commas.\\\n",
    "For instance, a list of technical skills for a specific project might appear as: ['JavaScript', 'React', 'Firebase'].\\\n",
    "5. If you cannot find the information corresponding to any of the keys, ensure that this key is still included in the JSON object.\\\n",
    "However, in such cases, assign its value as an empty Python list or empty string.\\\n",
    "For example, if no technical skills are associated with a particular project, you should include: 'technical_skills': [] in the JSON object.\\\n",
    "Similarly, if no employer or job title is found related to the project, you should include: 'employer': '', 'job_title': '' in the JSON object.\\\n",
    "6. Repeat these steps for each distinct project identified.\")\n",
    "\n",
    "response_schemas3=[project_schema]\n",
    "\n",
    "\n",
    "output_parser1 = StructuredOutputParser.from_response_schemas(response_schemas1)\n",
    "format_instructions1 = output_parser1.get_format_instructions()\n",
    "\n",
    "output_parser2 = StructuredOutputParser.from_response_schemas(response_schemas2)\n",
    "format_instructions2 = output_parser2.get_format_instructions()\n",
    "\n",
    "output_parser3 = StructuredOutputParser.from_response_schemas(response_schemas3)\n",
    "format_instructions3 = output_parser3.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_template1=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Extracts name in JSON format. \\\n",
    "    The keys must be 'name'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the name cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\n",
    "\n",
    "Extracts email in JSON format. \\\n",
    "    The keys must be 'email'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the email cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\n",
    "            \n",
    "Extracts phone number in JSON format. \\\n",
    "    The keys must be 'phone number'.Each corresponding value should be represented as a Python string. \\\n",
    "    If the phone number cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\n",
    "\n",
    "Extract information about the individual's educational background in JSON format.\\\n",
    "      Each educational experience should be represented as a separate JSON object. \\\n",
    "    For each education instance, the keys must be 'institution', 'degree_type', 'major', and 'graduation_date'. \\\n",
    "    Each corresponding value should be represented as a Python string.\\\n",
    "    If any information cannot be found for a given key, ensure the key is still included in the JSON object, but assign its corresponding value as null.\n",
    "\n",
    "Extracts details about professional and technical skills in JSON format. \\\n",
    "    The key must be 'skills'.\\\n",
    "    Each corresponding value should be represented as a Python list, with individual items separated by commas. \\\n",
    "    If certain information cannot be found, the corresponding key should still be included in the JSON object, but its value should be an empty Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions1} \"\"\"\n",
    "\n",
    "output_template2=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Follow steps below to extract work experiences: \n",
    "1. Begin by extracting details about each distinct job role from the work experience section.\\\n",
    "2. For every distinct job role, even if it is within the same company, create a separate JSON object.\\\n",
    "3. Each JSON object must include the following keys: 'job_title', 'employer', and 'employment_duration'.\\\n",
    "4. For the keys 'job_title', 'employer', and 'employment_duration', represent the corresponding values as Python strings.\\\n",
    "      For instance, the 'job_title' for a specific role might look like: 'Software Engineer'.\\\n",
    "5. If there is any key for which you cannot find the corresponding information, ensure that this key is still included in the JSON object. \\\n",
    "    If no details are found for the keys, assign their value as an empty Python string, for instance 'job_title': ''.\\\n",
    "6. Repeat these steps for each distinct job role identified in the work experience section.\n",
    "        \n",
    "text: {text}\n",
    "\n",
    "{format_instructions2} \"\"\"\n",
    "\n",
    "output_template3=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Follow the steps below to extract project details:\n",
    "1. Start by identifying and extracting details for each distinct project, the employer, the job title, and the technical skills utilized in each project.\n",
    "2. For each distinct project, create a separate JSON object.\n",
    "3. The JSON object for each project must include the following keys: 'project_name', 'employer', 'job_title', and 'technical_skills'.\n",
    "4. Represent the corresponding values for each key as Python lists or strings. \\\n",
    "Each individual item within the 'technical_skills' list should be separated by commas.\\\n",
    "For instance, a list of technical skills for a specific project might appear as: ['JavaScript', 'React', 'Firebase'].\n",
    "5. If you cannot find the information corresponding to any of the keys, ensure that this key is still included in the JSON object.\\\n",
    "However, in such cases, assign its value as an empty Python list or empty string.\\\n",
    "For example, if no technical skills are associated with a particular project, you should include: 'technical_skills': [] in the JSON object.\\\n",
    "Similarly, if no employer or job title is found related to the project, you should include: 'employer': '', 'job_title': '' in the JSON object.\n",
    "6. Repeat these steps for each distinct project identified.\n",
    "        \n",
    "text: {text}\n",
    "\n",
    "{format_instructions3} \"\"\"\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(template=output_template1)\n",
    "\n",
    "messages1 = prompt1.format_messages(text=resume_text, \n",
    "                                format_instructions1=format_instructions1)\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(template=output_template2)\n",
    "\n",
    "messages2 = prompt2.format_messages(text=resume_text, \n",
    "                                format_instructions2=format_instructions2)\n",
    "\n",
    "prompt3 = ChatPromptTemplate.from_template(template=output_template3)\n",
    "\n",
    "messages3 = prompt3.format_messages(text=resume_text, \n",
    "                                format_instructions3=format_instructions3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = chat(messages1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chat(messages2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = chat(messages3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict1 = output_parser1.parse(response1.content)\n",
    "output_dict2 = output_parser2.parse(response2.content)\n",
    "output_dict3 = output_parser3.parse(response3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'JAEWON YUN',\n",
       " 'email': 'jaewon.yun@mail.utoronto.ca',\n",
       " 'phone number': '4387285493',\n",
       " 'education': [{'institution': 'University of Toronto',\n",
       "   'degree_type': 'Physics Specialist (HBSc)',\n",
       "   'major': None,\n",
       "   'graduation_date': '2020'}],\n",
       " 'skills': ['Data Engineering/ETL Tools',\n",
       "  'Data Modeling',\n",
       "  'Data Warehousing',\n",
       "  'PySpark',\n",
       "  'Cypher Query Language',\n",
       "  'Docker',\n",
       "  'Databricks',\n",
       "  'Airflow',\n",
       "  'Graph Database (Neo4)j)',\n",
       "  'Git',\n",
       "  'Python',\n",
       "  'SQL/NoSQL Databases',\n",
       "  'SQL',\n",
       "  'Amazon Web Services',\n",
       "  'Redshift',\n",
       "  'SageMaker',\n",
       "  'Glue Studio',\n",
       "  'EC2',\n",
       "  'RDS',\n",
       "  'Lambda',\n",
       "  '$3',\n",
       "  'Cloud Development Kit',\n",
       "  'DynamoDB',\n",
       "  'IAM',\n",
       "  'Microsoft Azure',\n",
       "  'Synapse Analytics',\n",
       "  'Data Lake Storage',\n",
       "  'Data Factory',\n",
       "  'SQL Database',\n",
       "  'Blob Storage',\n",
       "  'Machine Learning',\n",
       "  'Natural Language Processing',\n",
       "  'Recommendation System',\n",
       "  'Statistical Modeling',\n",
       "  'Medical Image Processing',\n",
       "  'Time Series Analysis',\n",
       "  'TensorFlow/Keras']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work experiences': \"Follow steps below to extract work experiences: 1. Begin by extracting details about each distinct job role from the work experience section.2. For every distinct job role, even if it is within the same company, create a separate JSON object.3. Each JSON object must include the following keys: 'job_title', 'employer', and 'employment_duration'.4. For the keys 'job_title', 'employer', and 'employment_duration', represent the corresponding values as Python strings.      For instance, the 'job_title' for a specific role might look like: 'Software Engineer'.5. If there is any key for which you cannot find the corresponding information, ensure that this key is still included in the JSON object.     If no details are found for the keys, assign their value as an empty Python string, for instance 'job_title': ''.6. Repeat these steps for each distinct job role identified in the work experience section.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'projects': [{'project_name': 'End-to-End AWS Cloud Migration',\n",
       "   'employer': 'Branchy Solution 2',\n",
       "   'job_title': 'Data Engineer',\n",
       "   'technical_skills': ['AWS Glue Studio',\n",
       "    'Lambda',\n",
       "    '$3',\n",
       "    'Databricks',\n",
       "    'PySpark',\n",
       "    'AWS Redshift',\n",
       "    'RDS']},\n",
       "  {'project_name': 'Large Language Model-Powered Resume Rater Engine',\n",
       "   'employer': 'Unknown',\n",
       "   'job_title': 'Unknown',\n",
       "   'technical_skills': ['OpenAl',\n",
       "    'Resume Rater Engine',\n",
       "    'Apache Airflow',\n",
       "    'Docker',\n",
       "    'Neo4j',\n",
       "    'machine learning',\n",
       "    'predictive analytics']},\n",
       "  {'project_name': 'NLP-Powered Job Recommendation Engine with AWS',\n",
       "   'employer': 'Unknown',\n",
       "   'job_title': 'Unknown',\n",
       "   'technical_skills': ['NLP-powered job recommendation system',\n",
       "    'AWS SageMaker',\n",
       "    'EC2',\n",
       "    'APIs',\n",
       "    'Python',\n",
       "    '$3',\n",
       "    'DynamoDB',\n",
       "    'RDS']},\n",
       "  {'project_name': 'COVID Hospitalization Data Analysis Utilizing Microsoft Azure',\n",
       "   'employer': 'Unknown',\n",
       "   'job_title': 'Unknown',\n",
       "   'technical_skills': ['Synapse Analytics',\n",
       "    'SQL Database',\n",
       "    'Databricks',\n",
       "    'PySpark',\n",
       "    'ETL pipelines',\n",
       "    'Data Factory',\n",
       "    'Data Lake Storage',\n",
       "    'Blob Storage']}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'JAEWON YUN',\n",
       " 'email': 'jaewon.yun@mail.utoronto.ca',\n",
       " 'phone number': '4387285493',\n",
       " 'education': [{'institution': 'University of Toronto',\n",
       "   'degree_type': 'Physics Specialist (HBSc)',\n",
       "   'major': None,\n",
       "   'graduation_date': '2020'}],\n",
       " 'skills': ['Data Engineering/ETL Tools',\n",
       "  'Data Modeling',\n",
       "  'Data Warehousing',\n",
       "  'PySpark',\n",
       "  'Cypher Query Language',\n",
       "  'Docker',\n",
       "  'Databricks',\n",
       "  'Airflow',\n",
       "  'Graph Database (Neo4)j)',\n",
       "  'Git',\n",
       "  'Python',\n",
       "  'SQL/NoSQL Databases',\n",
       "  'SQL',\n",
       "  'Amazon Web Services',\n",
       "  'Redshift',\n",
       "  'SageMaker',\n",
       "  'Glue Studio',\n",
       "  'EC2',\n",
       "  'RDS',\n",
       "  'Lambda',\n",
       "  '$3',\n",
       "  'Cloud Development Kit',\n",
       "  'DynamoDB',\n",
       "  'IAM',\n",
       "  'Microsoft Azure',\n",
       "  'Synapse Analytics',\n",
       "  'Data Lake Storage',\n",
       "  'Data Factory',\n",
       "  'SQL Database',\n",
       "  'Blob Storage',\n",
       "  'Machine Learning',\n",
       "  'Natural Language Processing',\n",
       "  'Recommendation System',\n",
       "  'Statistical Modeling',\n",
       "  'Medical Image Processing',\n",
       "  'Time Series Analysis',\n",
       "  'TensorFlow/Keras'],\n",
       " 'work experiences': \"Follow steps below to extract work experiences: 1. Begin by extracting details about each distinct job role from the work experience section.2. For every distinct job role, even if it is within the same company, create a separate JSON object.3. Each JSON object must include the following keys: 'job_title', 'employer', and 'employment_duration'.4. For the keys 'job_title', 'employer', and 'employment_duration', represent the corresponding values as Python strings.      For instance, the 'job_title' for a specific role might look like: 'Software Engineer'.5. If there is any key for which you cannot find the corresponding information, ensure that this key is still included in the JSON object.     If no details are found for the keys, assign their value as an empty Python string, for instance 'job_title': ''.6. Repeat these steps for each distinct job role identified in the work experience section.\",\n",
       " 'projects': [{'project_name': 'End-to-End AWS Cloud Migration',\n",
       "   'employer': 'Branchy Solution 2',\n",
       "   'job_title': 'Data Engineer',\n",
       "   'technical_skills': ['AWS Glue Studio',\n",
       "    'Lambda',\n",
       "    '$3',\n",
       "    'Databricks',\n",
       "    'PySpark',\n",
       "    'AWS Redshift',\n",
       "    'RDS']},\n",
       "  {'project_name': 'Large Language Model-Powered Resume Rater Engine',\n",
       "   'employer': 'Unknown',\n",
       "   'job_title': 'Unknown',\n",
       "   'technical_skills': ['OpenAl',\n",
       "    'Resume Rater Engine',\n",
       "    'Apache Airflow',\n",
       "    'Docker',\n",
       "    'Neo4j',\n",
       "    'machine learning',\n",
       "    'predictive analytics']},\n",
       "  {'project_name': 'NLP-Powered Job Recommendation Engine with AWS',\n",
       "   'employer': 'Unknown',\n",
       "   'job_title': 'Unknown',\n",
       "   'technical_skills': ['NLP-powered job recommendation system',\n",
       "    'AWS SageMaker',\n",
       "    'EC2',\n",
       "    'APIs',\n",
       "    'Python',\n",
       "    '$3',\n",
       "    'DynamoDB',\n",
       "    'RDS']},\n",
       "  {'project_name': 'COVID Hospitalization Data Analysis Utilizing Microsoft Azure',\n",
       "   'employer': 'Unknown',\n",
       "   'job_title': 'Unknown',\n",
       "   'technical_skills': ['Synapse Analytics',\n",
       "    'SQL Database',\n",
       "    'Databricks',\n",
       "    'PySpark',\n",
       "    'ETL pipelines',\n",
       "    'Data Factory',\n",
       "    'Data Lake Storage',\n",
       "    'Blob Storage']}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_info=output_dict1 | output_dict2 | output_dict3\n",
    "resume_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_requirement={\"Minimum qualification\":\n",
    "                    {\"Degree\":\"Bachelor\",\n",
    "                     \"Major\":[\"Physics\", \"Computer Science\", \"Electrical Engineering\", \"Mathmetics\", \"Machine Learning\", \"Statistics\"],\n",
    "                     \"Skills\":[\"Statistical Analysis\"],\n",
    "                     \"Years of Experience\":\"3 years of experience\"},\n",
    "                \"Preferred qualification\":\n",
    "                    {\"Degree\":\"Phd\",\n",
    "                     \"Major\":[\"Computer Science\"],\n",
    "                     \"Skills\":[\"AWS\",\"Time Series Analysis\",\"Natural Language Processing\"],\n",
    "                     \"Years of Experience\":\"5 years of experience\"}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "# Establish connection\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"Apple1018!\"))  # replace with your details\n",
    "\n",
    "# Clear the graph for this example\n",
    "graph.delete_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume=resume_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_neo4j(resume):\n",
    "    try:\n",
    "        # Create the 'Person' node\n",
    "        person = Node(\"Person\", name=resume['name'], email=resume['email'], phone_number=resume['phone number'])\n",
    "        graph.create(person)\n",
    "\n",
    "        # Create the 'Education' nodes\n",
    "        for edu in resume['education']:\n",
    "            institution = Node(\"Institution\", name=edu['institution'])\n",
    "            graph.merge(institution, \"Institution\", \"name\")\n",
    "            \n",
    "            # Create 'Degree' node\n",
    "            degree = Node(\"DegreeType\", name=edu['degree_type'])\n",
    "            graph.merge(degree, \"DegreeType\", \"name\")\n",
    "\n",
    "            # 'studied_at' relation\n",
    "            rel = Relationship(person, \"STUDIED_AT\", institution)\n",
    "            graph.create(rel)\n",
    "\n",
    "            # 'graduated_in' relation\n",
    "            rel = Relationship(person, \"GRADUATED_IN\", institution, year=edu['graduation_date'])\n",
    "            graph.create(rel)\n",
    "\n",
    "            # 'obtained' relation\n",
    "            rel = Relationship(person, \"OBTAINED\", degree)\n",
    "            graph.create(rel)\n",
    "\n",
    "        # Create 'Skills' nodes\n",
    "        for skill in resume['skills']:\n",
    "            skill_node = Node(\"Skill\", name=skill)\n",
    "            graph.merge(skill_node, \"Skill\", \"name\")\n",
    "\n",
    "            # 'has_skill' relation\n",
    "            rel = Relationship(person, \"HAS_SKILL\", skill_node)\n",
    "            graph.create(rel)\n",
    "\n",
    "        # Create 'WorkExperience' nodes\n",
    "        for exp in resume['work experiences']:\n",
    "            employer = Node(\"Employer\", name=exp['employer'])\n",
    "            graph.merge(employer, \"Employer\", \"name\")\n",
    "            \n",
    "            job_title = Node(\"JobTitle\", title=exp['job_title'], duration=exp['employment_duration'])\n",
    "            graph.merge(job_title, \"JobTitle\", \"title\")\n",
    "\n",
    "            # 'held_position' relation\n",
    "            rel = Relationship(person, \"HELD_POSITION\", job_title)\n",
    "            graph.create(rel)\n",
    "            \n",
    "            # 'within' relation\n",
    "            rel = Relationship(job_title, \"WITHIN\", employer)\n",
    "            graph.create(rel)\n",
    "\n",
    "        # Create 'Project' nodes\n",
    "        for proj in resume['projects']:\n",
    "            project = Node(\"Project\", name=proj['project_name'])\n",
    "            graph.merge(project, \"Project\", \"name\")\n",
    "\n",
    "            # 'worked_on' relation\n",
    "            rel = Relationship(person, \"WORKED_ON\", project)\n",
    "            graph.create(rel)\n",
    "\n",
    "            for skill in proj['technical_skills']:\n",
    "                skill_node = Node(\"Skill\", name=skill)\n",
    "                graph.merge(skill_node, \"Skill\", \"name\")\n",
    "\n",
    "                # 'used_skill' relation\n",
    "                rel = Relationship(project, \"USED_SKILL\", skill_node)\n",
    "                graph.create(rel)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "save_to_neo4j(resume)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just use GPT3.5 without Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alan.pdf', '.DS_Store', 'jaewon.docx', 'oldresume.docx', 'jaeDE.pdf', 'lorraine.pdf', 'jaewon.pdf', 'mlscientist.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from util import extract_text_from_resume\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "def wraptext(text, width=100):\n",
    "    wrapper = textwrap.TextWrapper(width=width)\n",
    "    word_list = wrapper.wrap(text=text)\n",
    "\n",
    "    for element in word_list:\n",
    "        print(element)\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# Directory containing the resumes\n",
    "resume_dir = \"/Users/yunjaewon/ChatGPT/resumes/\"\n",
    "resume_list=os.listdir(resume_dir)\n",
    "print(resume_list)\n",
    "\n",
    "resume_text = extract_text_from_resume(resume_dir+'jaeDE.pdf')\n",
    "#print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    token_dict = {\n",
    "'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "'completion_tokens':response['usage']['completion_tokens'],\n",
    "'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
