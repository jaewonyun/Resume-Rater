{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from util import extract_text_from_resume\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_schema=ResponseSchema(name=\"name\",\n",
    "description=\"Extracts name in JSON format. \\\n",
    "    The keys must be 'name'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the name cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\")\n",
    "\n",
    "email_schema=ResponseSchema(name=\"email\",\n",
    "description=\"Extracts email in JSON format. \\\n",
    "    The keys must be 'email'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the email cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\")\n",
    "\n",
    "\n",
    "phone_number_schema=ResponseSchema(name=\"phone number\",\n",
    "description=\"Extracts phone number in JSON format. \\\n",
    "    The keys must be 'phone number'.Each corresponding value should be represented as a Python string. \\\n",
    "    If the phone number cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\")\n",
    "\n",
    "\n",
    "education_schema=ResponseSchema(name=\"education\",\n",
    "description=\"Extract information about the individual's educational background in JSON format.\\\n",
    "      Each educational experience should be represented as a separate JSON object. \\\n",
    "    For each education instance, the keys must be 'institution', 'degree_type', 'major', and 'graduation_date'. \\\n",
    "    Each corresponding value should be represented as a Python string.\\\n",
    "    If any information cannot be found for a given key, ensure the key is still included in the JSON object, but assign its corresponding value as null.\")\n",
    "\n",
    "\n",
    "work_experience_schema=ResponseSchema(name=\"work experiences\",\n",
    "description=\"Follow steps below to extract work experiences: \\\n",
    "1. Begin by extracting details about each distinct job role from the work experience section.\\\n",
    "2. For every distinct job role, even if it is within the same company, create a separate JSON object.\\\n",
    "3. Each JSON object must include the following keys: 'job_title', 'employer', and 'employment_duration'.\\\n",
    "4. For the keys 'job_title', 'employer', and 'employment_duration', represent the corresponding values as Python strings.\\\n",
    "      For instance, the 'job_title' for a specific role might look like: 'Software Engineer'.\\\n",
    "5. If there is any key for which you cannot find the corresponding information, ensure that this key is still included in the JSON object. \\\n",
    "    If no details are found for the keys, assign their value as an empty Python string, for instance 'job_title': ''.\\\n",
    "6. Repeat these steps for each distinct job role identified in the work experience section.\")\n",
    "\n",
    "\n",
    "project_schema=ResponseSchema(name=\"projects\",\n",
    "description=\"Follow the steps below to extract project details:\\\n",
    "1. Start by identifying and extracting details for each distinct project, the employer, the job title, and the technical skills utilized in each project.\\\n",
    "2. For each distinct project, create a separate JSON object.\\\n",
    "3. The JSON object for each project must include the following keys: 'project_name', 'employer', 'job_title', and 'technical_skills'.\\\n",
    "4. Represent the corresponding values for each key as Python lists or strings. \\\n",
    "Each individual item within the 'technical_skills' list should be separated by commas.\\\n",
    "For instance, a list of technical skills for a specific project might appear as: ['JavaScript', 'React', 'Firebase'].\\\n",
    "5. If you cannot find the information corresponding to any of the keys, ensure that this key is still included in the JSON object.\\\n",
    "However, in such cases, assign its value as an empty Python list or empty string.\\\n",
    "For example, if no technical skills are associated with a particular project, you should include: 'technical_skills': [] in the JSON object.\\\n",
    "Similarly, if no employer or job title is found related to the project, you should include: 'employer': '', 'job_title': '' in the JSON object.\\\n",
    "6. Repeat these steps for each distinct project identified.\")\n",
    "\n",
    "response_schemas1=[name_schema, email_schema, phone_number_schema, education_schema, work_experience_schema]\n",
    "\n",
    "response_schemas2=[project_schema]\n",
    "\n",
    "output_parser1 = StructuredOutputParser.from_response_schemas(response_schemas1)\n",
    "format_instructions1 = output_parser1.get_format_instructions()\n",
    "\n",
    "output_parser2 = StructuredOutputParser.from_response_schemas(response_schemas2)\n",
    "format_instructions2 = output_parser2.get_format_instructions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_template1=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Extracts name in JSON format. \\\n",
    "    The keys must be 'name'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the name cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\n",
    "\n",
    "Extracts email in JSON format. \\\n",
    "    The keys must be 'email'. Each corresponding value should be represented as a Python string.\\\n",
    "    If the email cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\n",
    "            \n",
    "Extracts phone number in JSON format. \\\n",
    "    The keys must be 'phone number'.Each corresponding value should be represented as a Python string. \\\n",
    "    If the phone number cannot be found, the key should still be included in the JSON object, but its corresponding value should be null.\n",
    "\n",
    "Extract information about the individual's educational background in JSON format.\\\n",
    "      Each educational experience should be represented as a separate JSON object. \\\n",
    "    For each education instance, the keys must be 'institution', 'degree_type', 'major', and 'graduation_date'. \\\n",
    "    Each corresponding value should be represented as a Python string.\\\n",
    "    If any information cannot be found for a given key, ensure the key is still included in the JSON object, but assign its corresponding value as null.\n",
    "\n",
    "Follow steps below to extract work experiences: \n",
    "1. Begin by extracting details about each distinct job role from the work experience section.\\\n",
    "2. For every distinct job role, even if it is within the same company, create a separate JSON object.\\\n",
    "3. Each JSON object must include the following keys: 'job_title', 'employer', and 'employment_duration'.\\\n",
    "4. For the keys 'job_title', 'employer', and 'employment_duration', represent the corresponding values as Python strings.\\\n",
    "      For instance, the 'job_title' for a specific role might look like: 'Software Engineer'.\\\n",
    "5. If there is any key for which you cannot find the corresponding information, ensure that this key is still included in the JSON object. \\\n",
    "    If no details are found for the keys, assign their value as an empty Python string, for instance 'job_title': ''.\\\n",
    "6. Repeat these steps for each distinct job role identified in the work experience section.\n",
    "\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions1} \"\"\"\n",
    "\n",
    "\n",
    "output_template2=\"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Follow the steps below to extract project details:\n",
    "1. Start by identifying and extracting details for each distinct project, the employer, the job title, and the technical skills utilized in each project.\n",
    "2. For each distinct project, create a separate JSON object.\n",
    "3. The JSON object for each project must include the following keys: 'project_name', 'employer', 'job_title', and 'technical_skills'.\n",
    "4. Represent the corresponding values for each key as Python lists or strings. \\\n",
    "Each individual item within the 'technical_skills' list should be separated by commas.\\\n",
    "For instance, a list of technical skills for a specific project might appear as: ['JavaScript', 'React', 'Firebase'].\n",
    "5. If you cannot find the information corresponding to any of the keys, ensure that this key is still included in the JSON object.\\\n",
    "However, in such cases, assign its value as an empty Python list or empty string.\\\n",
    "For example, if no technical skills are associated with a particular project, you should include: 'technical_skills': [] in the JSON object.\\\n",
    "Similarly, if no employer or job title is found related to the project, you should include: 'employer': '', 'job_title': '' in the JSON object.\n",
    "6. Repeat these steps for each distinct project identified.\n",
    "        \n",
    "text: {text}\n",
    "\n",
    "{format_instructions2} \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "resumes_dir = \"/Users/yunjaewon/JohnResume/resume-rater/resume_data/\"\n",
    "output_dir = \"/Users/yunjaewon/JohnResume/resume-rater/extracted_data/\"\n",
    "\n",
    "# List of all files in the directory\n",
    "resumes_files = os.listdir(resumes_dir)\n",
    "\n",
    "# Go through every file\n",
    "for filename in resumes_files:\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(resumes_dir, filename)\n",
    "\n",
    "    # Make sure it's a file and not a directory, and it's a .txt file (or whichever format your resumes are in)\n",
    "    if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                resume = file.read()\n",
    "                # Process the content here\n",
    "                # ...\n",
    "\n",
    "                prompt1 = ChatPromptTemplate.from_template(template=output_template1)\n",
    "                messages1 = prompt1.format_messages(text=resume, format_instructions1=format_instructions1)\n",
    "                response1 = chat(messages1, temperature=0.0)\n",
    "                output_dict1 = output_parser1.parse(response1.content)\n",
    "\n",
    "                prompt2 = ChatPromptTemplate.from_template(template=output_template2)\n",
    "                messages2 = prompt2.format_messages(text=resume, format_instructions2=format_instructions2)\n",
    "                response2 = chat(messages2, temperature=0.0)\n",
    "                output_dict2 = output_parser2.parse(response2.content)\n",
    "\n",
    "                \n",
    "\n",
    "                # Combine the outputs into one dictionary\n",
    "                output_dict = output_dict1 | output_dict2\n",
    "\n",
    "                # Construct the output file path, preserving the original file name but changing the directory and extension\n",
    "                output_file_path = os.path.join(output_dir, os.path.splitext(filename)[0] + '.txt')\n",
    "\n",
    "                # Write the output to a file in the output directory\n",
    "                with open(output_file_path, 'w') as output_file:\n",
    "                    json.dump(output_dict, output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "# Now you have a .txt file in the output directory for each resume, containing the processed information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_files=os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes_files = [f for f in os.listdir(resumes_dir) if f != '.DS_Store']\n",
    "\n",
    "sorted(extracted_files)==sorted(resumes_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from util import extract_text_from_resume\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# Set OpenAI API key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "output_template1=\"\"\"\\\n",
    "Given the text provided, please perform the following actions:\n",
    "\n",
    "Closely examine the text for information related to work experience. For each employment instance, strive to extract the following details:\n",
    "\n",
    "    - 'Job Title': The role or position held by the individual.\n",
    "    - 'Employer': The organization or company where the individual was employed.\n",
    "    - 'Project': Specific tasks, responsibilities, or initiatives that the individual was a part of or led during their employment. \n",
    "    - 'Skills Used': List any particular abilities, knowledge, or competencies that the individual employed during their projects or job tasks.\n",
    "    - 'Employment Tenure Assessment': Please calculate the length of employment tenure, derived from the given start and end dates. \\\n",
    "        Your result should specify the duration in terms of years and months. For example, '3 years and 4 months'.\n",
    "\n",
    "If the individual held multiple positions within the same company, ensure that details for each position are separately presented.\n",
    "\n",
    "Text for Analysis: {text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "output_template2=\"\"\"\\\n",
    "Given the provided text, please carry out the following tasks:\n",
    "\n",
    "1. Scan the entirety of the text, including but not limited to the 'Skills' section, to identify and extract any explicitly mentioned technical skills. This may include specific software proficiencies, technical abilities, certifications, or other specialised competencies.\n",
    "\n",
    "2. Additionally, examine the sections related to work experiences. For each employment instance, identify and extract any technical skills that may be implied or directly stated in the description of job roles, responsibilities, or projects. \n",
    "\n",
    "3. Present these skills in a consolidated and organised manner, clearly differentiating between skills explicitly stated and those inferred from work experiences.\n",
    "\n",
    "Text for Analysis: {text} \"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "resumes_dir = \"/Users/yunjaewon/JohnResume/resume-rater/resume_data/\"\n",
    "skill_output_dir = \"/Users/yunjaewon/JohnResume/resume-rater/skills_data/\"\n",
    "work_output_dir=\"/Users/yunjaewon/JohnResume/resume-rater/work_data/\"\n",
    "\n",
    "# List of all files in the directory\n",
    "resumes_files = os.listdir(resumes_dir)\n",
    "\n",
    "# Go through every file\n",
    "for filename in resumes_files:\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(resumes_dir, filename)\n",
    "\n",
    "    # Make sure it's a file and not a directory, and it's a .txt file (or whichever format your resumes are in)\n",
    "    if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                resume = file.read()\n",
    "                # Process the content here\n",
    "                # ...\n",
    "\n",
    "                prompt1 = ChatPromptTemplate.from_template(template=output_template1)\n",
    "                messages1 = prompt1.format_messages(text=resume)\n",
    "                response1 = chat(messages1, temperature=0.0)\n",
    "                \n",
    "\n",
    "                prompt2 = ChatPromptTemplate.from_template(template=output_template2)\n",
    "                messages2 = prompt2.format_messages(text=resume)\n",
    "                response2 = chat(messages2, temperature=0.0)\n",
    "                \n",
    "\n",
    "                \n",
    "\n",
    "                # Construct the output file path, preserving the original file name but changing the directory and extension\n",
    "                #work\n",
    "                output_file_path1 = os.path.join(work_output_dir, os.path.splitext(filename)[0] + '_work.txt')\n",
    "                #skill\n",
    "                output_file_path2 = os.path.join(skill_output_dir, os.path.splitext(filename)[0] + '_skills.txt')\n",
    "                # Write the output to a file in the output directory\n",
    "                with open(output_file_path1, 'w') as output_file:\n",
    "                    json.dump(response1.content, output_file)\n",
    "                with open(output_file_path2, 'w') as output_file:\n",
    "                    json.dump(response2.content, output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "# Now you have a .txt file in the output directory for each resume, containing the processed information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: AWS Data Engineer\n",
      "Employer: FedEx Corporation\n",
      "Project: \n",
      "- Developed and maintained data pipelines using technologies such as Mariadb, Google Storage, Snowflake, ETL frameworks, and Hadoop.\n",
      "- Collaborated with cross-functional teams to optimize data storage and retrieval processes.\n",
      "- Assisted in the implementation and management of AWS-based data solutions.\n",
      "Skills Used: Data Engineering, AWS, Mariadb, Google Storage, Snowflake, ETL Frameworks, Hadoop\n",
      "Employment Tenure Assessment: 2 years and 9 months\n",
      "\n",
      "Job Title: AWS Data Engineer\n",
      "Employer: Microsoft Corporation\n",
      "Project: \n",
      "- Designed and implemented data processing systems using Mariadb, Google Storage, Snowflake, ETL frameworks, and Hadoop.\n",
      "- Worked closely with data scientists to transform raw data into valuable insights.\n",
      "- Contributed to the development of scalable and efficient data infrastructure.\n",
      "Skills Used: Data Engineering, AWS, Mariadb, Google Storage, Snowflake, ETL Frameworks, Hadoop\n",
      "Employment Tenure Assessment: 4 years and 5 months\n",
      "\n",
      "Job Title: Kotlin Developer\n",
      "Employer: Prologis, Inc.\n",
      "Project: \n",
      "- Developed high-quality software solutions using Kotlin, GraphQL, SQL Server, REST APIs, and HTML/CSS.\n",
      "- Collaborated with a team to build web applications that enhanced the company's productivity and efficiency.\n",
      "- Provided technical guidance and support to junior developers.\n",
      "Skills Used: Kotlin Development, GraphQL, SQL Server, REST APIs, HTML/CSS\n",
      "Employment Tenure Assessment: 4 years, 9 months\n",
      "\n",
      "Job Title: Artificial Intelligence Engineer\n",
      "Employer: Accenture plc\n",
      "Project: \n",
      "- Applied advanced AI techniques such as random forest, neural networks, Bayesian networks, convolutional neural networks, and time series analysis to solve complex business problems.\n",
      "- Developed AI models and algorithms to optimize various processes and improve decision-making.\n",
      "- Assisted in the deployment and integration of AI solutions within client organizations.\n",
      "Skills Used: Artificial Intelligence, Random Forest, Neural Networks, Bayesian Networks, Convolutional Neural Networks, Time Series Analysis\n",
      "Employment Tenure Assessment: 3 years and 5 months\n"
     ]
    }
   ],
   "source": [
    "print(response1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicitly mentioned technical skills:\n",
      "- Software Engineering\n",
      "- Data Engineering\n",
      "- Kotlin Development\n",
      "- Artificial Intelligence\n",
      "- AWS\n",
      "- MariaDB\n",
      "- Google Storage\n",
      "- Snowflake\n",
      "- ETL Frameworks\n",
      "- Hadoop\n",
      "- GraphQL\n",
      "- SQL Server\n",
      "- REST APIs\n",
      "- HTML/CSS\n",
      "- Random Forest\n",
      "- Neural Networks\n",
      "- Bayesian Networks\n",
      "- Convolutional Neural Networks\n",
      "- Time Series Analysis\n",
      "\n",
      "Technical skills inferred from work experiences:\n",
      "- Mariadb\n",
      "- Google Storage\n",
      "- Snowflake\n",
      "- ETL frameworks\n",
      "- Hadoop\n",
      "- Kotlin\n",
      "- GraphQL\n",
      "- SQL Server\n",
      "- REST APIs\n",
      "- HTML/CSS\n",
      "- Random Forest\n",
      "- Neural Networks\n",
      "- Bayesian Networks\n",
      "- Convolutional Neural Networks\n",
      "- Time Series Analysis\n",
      "\n",
      "Consolidated list of technical skills:\n",
      "- Software Engineering\n",
      "- Data Engineering\n",
      "- Kotlin Development\n",
      "- Artificial Intelligence\n",
      "- AWS\n",
      "- MariaDB\n",
      "- Google Storage\n",
      "- Snowflake\n",
      "- ETL Frameworks\n",
      "- Hadoop\n",
      "- GraphQL\n",
      "- SQL Server\n",
      "- REST APIs\n",
      "- HTML/CSS\n",
      "- Random Forest\n",
      "- Neural Networks\n",
      "- Bayesian Networks\n",
      "- Convolutional Neural Networks\n",
      "- Time Series Analysis\n"
     ]
    }
   ],
   "source": [
    "print(response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johannes Gutenberg\n",
      "Master of Science in Software Engineering | AWS Data Engineer | Kotlin Developer | Artificial Intelligence Engineer\n",
      "\n",
      "Contact Information:\n",
      "Email: johannes.gutenberg@example.com\n",
      "Phone: (123) 456-7890\n",
      "LinkedIn: www.linkedin.com/in/johannesgutenberg\n",
      "\n",
      "Summary:\n",
      "Highly skilled and adaptable professional with a diverse background in software engineering, data engineering, Kotlin development, and artificial intelligence. Possesses a Master of Science in Software Engineering from Duke University and a strong track record of acquiring new skills and taking on challenging projects. Excel in collaborating with cross-functional teams to drive innovation and deliver exceptional results.\n",
      "\n",
      "Education:\n",
      "Master of Science in Software Engineering\n",
      "Duke University, Durham, NC\n",
      "Graduated: 2001\n",
      "\n",
      "Skills:\n",
      "- Software Engineering\n",
      "- Data Engineering\n",
      "- Kotlin Development\n",
      "- Artificial Intelligence\n",
      "- AWS\n",
      "- MariaDB\n",
      "- Google Storage\n",
      "- Snowflake\n",
      "- ETL Frameworks\n",
      "- Hadoop\n",
      "- GraphQL\n",
      "- SQL Server\n",
      "- REST APIs\n",
      "- HTML/CSS\n",
      "- Random Forest\n",
      "- Neural Networks\n",
      "- Bayesian Networks\n",
      "- Convolutional Neural Networks\n",
      "- Time Series Analysis\n",
      "\n",
      "Experience:\n",
      "\n",
      "AWS Data Engineer\n",
      "FedEx Corporation, Memphis, TN\n",
      "2000-01 - 2002-10\n",
      "- Developed and maintained data pipelines using technologies such as Mariadb, Google Storage, Snowflake, ETL frameworks, and Hadoop.\n",
      "- Collaborated with cross-functional teams to optimize data storage and retrieval processes.\n",
      "- Assisted in the implementation and management of AWS-based data solutions.\n",
      "\n",
      "AWS Data Engineer\n",
      "Microsoft Corporation, Redmond, WA\n",
      "2004-11 - 2009-04\n",
      "- Designed and implemented data processing systems using Mariadb, Google Storage, Snowflake, ETL frameworks, and Hadoop.\n",
      "- Worked closely with data scientists to transform raw data into valuable insights.\n",
      "- Contributed to the development of scalable and efficient data infrastructure.\n",
      "\n",
      "Kotlin Developer\n",
      "Prologis, Inc., San Francisco, CA\n",
      "1997-05 - 2002-03\n",
      "- Developed high-quality software solutions using Kotlin, GraphQL, SQL Server, REST APIs, and HTML/CSS.\n",
      "- Collaborated with a team to build web applications that enhanced the company's productivity and efficiency.\n",
      "- Provided technical guidance and support to junior developers.\n",
      "\n",
      "Artificial Intelligence Engineer\n",
      "Accenture plc, New York, NY\n",
      "1999-07 - 2002-12\n",
      "- Applied advanced AI techniques such as random forest, neural networks, Bayesian networks, convolutional neural networks, and time series analysis to solve complex business problems.\n",
      "- Developed AI models and algorithms to optimize various processes and improve decision-making.\n",
      "- Assisted in the deployment and integration of AI solutions within client organizations.\n",
      "\n",
      "Additional Information:\n",
      "- Strong communication and collaboration skills.\n",
      "- Proven ability to adapt to new technologies and learn quickly.\n",
      "- Passionate about innovation and staying informed about the latest industry trends.\n",
      "- Fluent in English and German.\n",
      "\n",
      "References:\n",
      "Available upon request.\n"
     ]
    }
   ],
   "source": [
    "print(resume)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
